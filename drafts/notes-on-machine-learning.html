<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Pure Wild Animal Craziness, Personal Homepage of Chris Scutcher, Esq.">

        <link rel="alternate"  href="http://chris.scutcher.uk/feeds/all.atom.xml" type="application/atom+xml" title="Pure Wild Animal Craziness Full Atom Feed"/>
        <link rel="alternate" href="http://chris.scutcher.uk/feeds/all.rss.xml" type="application/rss+xml" title="Pure Wild Animal Craziness Full RSS Feed"/>

        <title>Notes on MachineÂ Learning // Pure Wild Animal Craziness // Personal Homepage of Chris Scutcher, Esq.</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://chris.scutcher.uk/theme/css/pure.css">
    <link rel="stylesheet" href="http://chris.scutcher.uk/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>

    <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      "headline": "Notes on Machine&nbsp;Learning",
      "dateCreated": "2016-03-28 16:52:27+01:00",
      "datePublished": "2016-04-09 19:51:31+01:00",
      "description": "Notes on Machine&nbsp;Learning",
      "keywords": "machine-learning,notes,",
      "author": {
          "@context": "http://schema.org",
          "@type": "Person",
          "url": "http://chris.scutcher.uk/pages/about_me"
      }
    }
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background-image: url('/images/cover_img.png')">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <img class="avatar" src="https://www.gravatar.com/avatar/7dcb6b43b700a2870faa3aa1941d6da4?s=200&r=pg&d=mm">
                            <h1 class="brand-main"><a href="http://chris.scutcher.uk">Pure Wild Animal Craziness</a></h1>
                            <p class="tagline">Personal Homepage of Chris Scutcher, Esq.</p>
                                <p class="links"><a href="/category/blog">Blog</a></p>
                                <p class="links"><a href="/category/snippets">Snippets</a></p>
                                <p class="links"><a href="/pages/about_me">About</a></p>
                                <p class="links"><a href="/pages/cv">CV</a></p>
                                <p class="social">
                                    <a href="https://github.com/cscutcher">
                                        <i class="fa fa-github fa-3x"></i>
                                    </a>
                                    <a href="https://uk.linkedin.com/in/cscutcher">
                                        <i class="fa fa-linkedin fa-3x"></i>
                                    </a>
                                    <a href="https://plus.google.com/+ChrisScutcher">
                                        <i class="fa fa-google-plus fa-3x"></i>
                                    </a>
                                    <a href="https://steamcommunity.com/id/zoolie/">
                                        <i class="fa fa-steam fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Notes on Machine&nbsp;Learning</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="http://chris.scutcher.uk/tag/machine-learning.html">machine-learning</a>
                                <a class="post-category" href="http://chris.scutcher.uk/tag/notes.html">notes</a>
                            Last modified: 2016-04-09 19:51:31+01:00
                        </p>
                </header>
            </section>
            <p>This page contains some notes written when taking the excellent 
<a href="https://www.coursera.org/learn/machine-learning/">Coursera Machine Learning Course from Stanford</a>&nbsp;.</p>
<p>I&#8217;ve almost certainly made some mistakes but I figured I&#8217;d write them here for
my convenience and publish them in case others find them&nbsp;useful.</p>
<p>[<span class="caps">TOC</span>]</p>
<h1>Linear&nbsp;Regression</h1>
<p><strong> <a href="https://www.coursera.org/learn/machine-learning/home/week/2">See week&nbsp;2</a> </strong> </p>
<p>$$
h_\theta(x) = \sum_{j=0}^n \theta_j x_j \
h_\theta(x) = \theta^T x&nbsp;$$</p>
<p>$$
J(\theta) = \frac{1}{2m} \left[ { \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} ) ^ 2} \right]   \
\theta_j := \theta_j - \alpha\frac{1}{m}\left[ \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\right]<br>&nbsp;$$</p>
<h1>Logistic&nbsp;Regression</h1>
<p><strong> <a href="https://www.coursera.org/learn/machine-learning/home/week/3">See week&nbsp;3</a> </strong> </p>
<ul>
<li>Linear regression won&#8217;t&nbsp;work!</li>
<li>Sigmoid function ensures convex cost&nbsp;graph</li>
<li>$h_\theta(x)=P(y=1|x;0)$</li>
</ul>
<p>$$
h_\theta(x) = g(\theta ^ Tx)\
g(z) = \frac{1}{1+e^{-z}}\
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}&nbsp;$$</p>
<h1>Dealing with fitting with&nbsp;Regularization</h1>
<p><strong> <a href="https://www.coursera.org/learn/machine-learning/home/week/4">See week&nbsp;4</a> </strong> </p>
<h2>Data&nbsp;fitting</h2>
<p><strong>Underfitting</strong> : &#8216;High bias&#8217; when learned hypothesis does not fit very&nbsp;well. </p>
<p><strong>Overfitting</strong>: &#8216;High variance&#8217; when the learned hypothesis adheres too closely
to training&nbsp;data.</p>
<p>If we have too many <strong>features</strong> the learned hypothesis may fit training set
very well, but fail to generalise to new&nbsp;examples.</p>
<h2>How to address&nbsp;overfitting</h2>
<ul>
<li>Reduce number of&nbsp;features.</li>
<li>Regularization;<ul>
<li>Keep all features but reduce their&nbsp;magnitude.</li>
<li>Works well with many&nbsp;features.</li>
</ul>
</li>
</ul>
<h2>Regularization to deal with&nbsp;overfitting</h2>
<p>Works by optimizing down magnitude of $\theta$ in addition to the cost which is
already being&nbsp;.</p>
<h2>Regularizing Linear&nbsp;Regression</h2>
<p>$$
J(\theta) = \frac{1}{2m} \left[ \color{green} { \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} ) ^ 2} + \color{red}{\leftthreetimes \sum_{i=1}^{N} \theta_j^2 } \right]&nbsp;$$</p>
<p>$$
\theta_j := \theta_j - \alpha\left[\frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\leftthreetimes}{m}\theta_j\right]&nbsp;$$</p>
<p>or rewritten&nbsp;as</p>
<p>$$
\theta_j := \theta_j \left(1-\alpha \frac{\leftthreetimes}{m} \right) - \alpha \frac{1}{m} \sum_{i=2}^m \left(h_\theta ( x^{(i)} ) - y^{(i)} \right) x_j^{(i)}&nbsp;$$</p>
<p><strong> Do not regularise&nbsp;$\theta_0$ </strong></p>
<p>Green is the original cost parameter. It&#8217;s goal is to make the hypothesis fit
the training&nbsp;data.</p>
<p>Red is the regularization parameter. It&#8217;s goal is to simplify the hypothesis by
reducing the magnitude of&nbsp;$\theta$.</p>
<h2>Regularizing Linear Regression with Normal&nbsp;Equation</h2>
<p><span class="caps">TODO</span>: Notes on regularizing normal&nbsp;equation</p>
<h2>Regularizing Logistic&nbsp;Regression</h2>
<p>$$
J(\theta) = \left[ -\frac{1}{m} \sum_{i=1}^{n} y^{(i)} \log(h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta (x^{(i)}) \right] + \frac{\leftthreetimes}{2m} \sum_{j=1}^{m} \theta_j^2&nbsp;$$</p>
<p>$$
\text{grad} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\leftthreetimes}{m} \theta_j&nbsp;$$</p>
<h2>Effect of&nbsp;$\leftthreetimes$</h2>
<p>If $\leftthreetimes$ is <strong>too high</strong>  then we will see an&nbsp;underfit.</p>
<p>If $\leftthreetimes$ is <strong>too low</strong>  then we will see an&nbsp;overfit.</p>
<h1>Neural&nbsp;Networks</h1>
<p><strong> <a href="https://www.coursera.org/learn/machine-learning/home/week/4">See week 4</a>
and exercise&nbsp;3 </strong> </p>
<ul>
<li>Well suited to non-linear problems that would require many&nbsp;features.</li>
<li>Feature complexity for scales at something like $O(n^2)$ or&nbsp;$O\left(\frac{n^2}{2}\right)$</li>
<li>Popular in 1980-1990s, but also seen resurgence&nbsp;recently.</li>
<li>State of the art for many&nbsp;applications.</li>
</ul>
<h2>Representation</h2>
<p>Here&#8217;s a couple of slides that are easier to just include rather than&nbsp;rewrite;</p>
<p><img alt="Slide introducing non-vectorized NN representation" src="http://chris.scutcher.uk/images/machine_learning_coursera_nn_0.png"></p>
<p><img alt="Slide introducing vectorized NN representation" src="http://chris.scutcher.uk/images/machine_learning_coursera_nn_1.png"></p>
<p>If network has $s_j$ units in layer $j$ and $s_{(j+1)}$ in layer $j+1$
then $\theta(j)$ will be dimensions;
$$
s_{(j+1)} \times (s_j + 1)&nbsp;$$</p>
<h2>Lingo</h2>
<ul>
<li>$x_0$ = bias&nbsp;unit</li>
<li>Sigmoid/Logistic <strong>Activation&nbsp;Function</strong></li>
<li>$\theta$ == weights ==&nbsp;parameters</li>
<li>Layer 1 = Input Layer. Layer [-1] = Output Layer. Layer [others] = Hidden&nbsp;layer.</li>
<li>$a_i^{(j)}$ is &#8220;activating of unit $i$ in layer&nbsp;$j$.</li>
<li>$\theta{(j)}$ is matrix of weights controlling function mapping from layer
  $j$ to layer&nbsp;$j+1$.</li>
</ul>
<h2>Cost&nbsp;Function</h2>
<p>$$
J(\theta)= \frac{1}{m}\sum^m_{i=1}\sum^K_{k=1}\left[ - y_k^{(i)}\log \left( \left( h_\theta ( x^{(i)} ) \right)k \right) - \left( 1 - y_k^{(i)} \right) \log \left( 1 - \left( h_\theta(x^{(i)}) \right)  k\right) \right] + \
\frac{\leftthreetimes}{2m}\left[ \sum^{25}<em k="1">{j=1}\sum^{400}</em> \left( \theta_{j,k}^{(1)} \right)^2 + \sum^{10}<em k="1">{j=1}\sum^{25}</em> \left( \theta_{j,k}^{(2)} \right)^2 \right]&nbsp;$$</p>
<p><strong> Note that $J(\theta)$ is non-convex and may converge on lcal&nbsp;minima </strong></p>
<h2>Grad Function/Back&nbsp;Prop</h2>
<p>$$
\text{sigmoid}(z)=g(z)=\frac{1}{1+e^{-z}}\
g&#8217;(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))&nbsp;$$</p>
<ol>
<li>Forward propagate to get activation values for all&nbsp;units.</li>
<li>Calculate $\delta$ for output unit: 
$$
\delta_k^{\text{end}} = (a_k^{(\text{end})} - y_k)&nbsp;$$</li>
<li>For all other layers;
$$
\delta^{(l)} = (\theta^{(l)})^T\delta^{(l+1)}(a^{(l)})^T&nbsp;$$</li>
<li>Accumulate the gradient with following formula.
$$
\triangle^{(l)} = \triangle^{(l)} + \delta^{(l+1)}(a^{(l)})^T
$$
Remove&nbsp;$\delta_0^{l}$.</li>
<li>Obtain the gradient for $j=0$:
$$
\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)}=\frac{1}{m}\triangle_{ij}^{(l)}
$$
for $j&gt;=1$
$$
\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)}=\frac{1}{m}\triangle_{ij}^{(l)} + \frac{\leftthreetimes}{m}\theta_{ij}^{(l)}&nbsp;$$</li>
</ol>
<h2>Initialising&nbsp;$\theta$</h2>
<p><strong> Don&#8217;t initialise to all the same value as nn will get <em>stuck</em> </strong></p>
<p>Initialise each to random value between $\epsilon$ and&nbsp;$-\epsilon$.</p>
<h2>Architecture</h2>
<p>When choosing architecture a reasonable initial step is to choose 3 layer&nbsp;design</p>
<ul>
<li>Input&nbsp;layer</li>
<li>Hidden&nbsp;layer</li>
<li>Output&nbsp;layer</li>
</ul>
<p>If using more layers (say $n$) start with same units per layer.
More units per layer is usually <em>better</em>.</p>
<h2>Gradient&nbsp;Checking</h2>
<p>Gradient checking can be used to check implementation.
It should not be used exhaustively as it&#8217;s too expensive to&nbsp;calculate. </p>
<p>$$
f_i(\theta) \approx \frac{J(\theta^{i+\epsilon}) - J(\theta^{i-\epsilon})}{2\epsilon}&nbsp;$$</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">n</span>
    <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
    <span class="n">thetaPlus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaPlus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">;</span>
    <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
    <span class="n">thetaMinus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">thetaMinus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">-</span> <span class="n">EPSILON</span><span class="p">;</span>
    <span class="n">gradApprox</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> <span class="o">-</span> <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">EPSILON</span><span class="p">)</span>
<span class="k">end</span><span class="p">;</span>
</pre></div>


<p>$\text{gradApprox} \approx&nbsp;\text{DVec}$</p>
<h2>Excercise 4&nbsp;Results</h2>
<p><img alt="Excercise 4 Output" src="http://chris.scutcher.uk/images/machine_learning_ex4_output.png"></p>
<ul>
<li>50 iterations. 96.14%&nbsp;accuracy.</li>
<li>250 iterations. 99.36%&nbsp;accuracy.</li>
<li>500 iterations. 99.4%&nbsp;accuracy.</li>
</ul>
<h2>On actual (biological)&nbsp;neurons</h2>
<ul>
<li>Dendrite is&nbsp;&#8216;input&#8217;</li>
<li>Axon is&nbsp;&#8216;output&#8217;</li>
</ul>
            <a href="#" class="go-top">Go Top</a>
    <script src="https://apis.google.com/js/plusone.js">
    </script>
    <div class="comments">
        <div id='gpluscomments'></div>
        <script src='https://apis.google.com/js/plusone.js' type='text/javascript'></script>
        <script>
            gapi.comments.render(
                'gpluscomments',
                {
                    href: window.location.href.replace(
                                  // This nasty hack is because the g+ commen
                                  // wotsit doesn't seem to link slightly
                                  // differing hostnames.
                                  window.location.pathname, '/permalinks/XNF5Voe2B7mLfS1N1n2M0xkQVd8='),
                    first_party_property: 'BLOGGER',
                    view_type: 'FILTERED_POSTMOD'
                })
        </script>
    </div>
<footer class="footer">
    <p>
        &copy; Chris Scutcher &ndash;
        <a href=http://chris.scutcher.uk//permalinks/XNF5Voe2B7mLfS1N1n2M0xkQVd8=>Permalink</a> &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

</body>
</html>